# BlenDA: Domain Adaptive Object Detection through Diffusion Blending

![teaser_a](https://github.com/michaelku1/semantic_alignment_aiiulab/assets/48415065/20c1c4cb-e20c-4b55-aa8e-c4a876800b20)


## Abstract

Unsupervised domain adaptation (UDA) aims to transfer a model learned using the labeled data from the source domain to the unla- beled data in the target domain. To address the large domain gap is- sue between the source and target domains, we propose a novel regu- larization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The inter- mediate samples are generated by dynamically blending the source images and their corresponding translated images using the off-the- shelf pretrained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. From the experimental results on two adaptation benchmarks, the proposed approach can signifi- cantly enhance the performance of the state-of-the-art domain adap- tive object detector, Adversarial Query Transformer (AQT). Particu- larly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpass- ing the previous state-of-the-art by 1.5%. It is worth noting that our proposed regularization is also applicable to various paradigms of domain adaptive object detection.


## Contributions:
- We are the first to consider diffusion-based synthesized images to retain semantics of small objects, thus becoming robust against domain shift.
- We leverage a mix-up training method and introduces a dynamic weighting scheme to stabilize domain transfer.
- Experimental results show that our method surpasses previous Faster-RCNN based and Transformer based methods in both cityscapes to foggy cityscapes, cityscapes to BDD100k daytime settings

## Dataset
- [cityscapes](https://www.cityscapes-dataset.com/)
- [BDD100k](https://www.vis.xyz/bdd100k/)


## Training and Evaluation

start from pretrain:

`CUDA_VISIBLE_DEVICES=0 GPUS_PER_NODE=1 ./tools/run_d/run_dist_launch.sh 1 --master_port 29116 python main.py --config_file configs/contrastive_from_39_epochs.yaml --opts OUTPUTexps_s_DIR exps_bs4_retrain/debug TRAIN.EPOCHS 102 DATASET.NUM_CLASSES 9 TRAIN.BATCH_SIZE 4 MODEL.STAGE train_AQT DATASET.DA_MODE NUM_FTuda MODEL.NUM_FEATURE_LEVELS 1 RESUME exps_bs4_retrain/AQT_pretrain/checkpoint0040.pth FINETUNE True EMA True CONTRASTIVE True LASS_Fue LOSS.INTER_CLASS_COEF 1. LOSS.INTRA_CLASS_COEF 1.`

evaluation:

`CUDA_VISIBLE_DEVICES=0 GPUS_PER_NODE=1 ./tools/run_dist_launch.sh 1 --master_port 29110 python main.py --config_file configs/contrastive.yaml --opts OUTPUT_DIR ./exps/exp1/contrastive TRAIN.EPOCHS 100 DATASET.NUM_CLASSES 9 TRAIN.BATCH_SIZE 2 FINETUNE False RESUME ${MDOEL_CHECKPONIT_PATH.pth} EVAL True`


## Ackowledgement
We thank the developers and authors of [AQT](https://github.com/weii41392/AQT) for releasing their helpful codebases.



