{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a430de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import numpy\n",
    "import numpy as np\n",
    "\n",
    "#Import scikitlearn for machine learning functionalities\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits # For the UCI ML handwritten digits dataset\n",
    "\n",
    "# Import matplotlib for plotting graphs ans seaborn for attractive graphics.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import datasets.DAOD as DAOD\n",
    "import util.misc as utils\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "import models\n",
    "from models import build_model\n",
    "\n",
    "from config import get_cfg_defaults\n",
    "from tqdm import tqdm\n",
    "\n",
    "import util.misc as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b521bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/scratch2/users/cku/adaptation/AQT_subset/models/__init__.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models) # reload folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c638f",
   "metadata": {},
   "source": [
    "##  config set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c29c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best way is to modify all args to manual parameter\n",
    "\n",
    "def setup(config_file):\n",
    "    # initialise cfg from both defualt and the one defined by file\n",
    "    cfg = get_cfg_defaults()\n",
    "    \n",
    "    if config_file:\n",
    "        cfg.merge_from_file(config_file)\n",
    "#     if opts:\n",
    "#         cfg.merge_from_list(args.opts)\n",
    "    utils.init_distributed_mode(cfg)\n",
    "    cfg.freeze()\n",
    "    \n",
    "    # copy backup scripts for ease of debugging\n",
    "    if cfg.OUTPUT_DIR:\n",
    "        Path(cfg.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        os.system(f'cp {config_file} {cfg.OUTPUT_DIR}')\n",
    "        ddetr_src = 'models/deformable_detr.py'\n",
    "        ddetr_des = Path(cfg.OUTPUT_DIR) / 'deformable_detr.py.backup'\n",
    "        dtrans_src = 'models/deformable_transformer.py'\n",
    "        dtrans_des = Path(cfg.OUTPUT_DIR) / 'deformable_transformer.py.backup'\n",
    "        main_src = 'main.py'\n",
    "        main_des = Path(cfg.OUTPUT_DIR) / 'main.py.backup'\n",
    "        os.system(f'cp {ddetr_src} {ddetr_des}')\n",
    "        os.system(f'cp {dtrans_src} {dtrans_des}')\n",
    "        os.system(f'cp {main_src} {main_des}')\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# write a name matching function, if names match named_params \n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60e8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "# config_file = 'configs/r50_uda_c2fc.yaml'\n",
    "\n",
    "config_file = 'configs/debug_mode.yaml'\n",
    "cfg = setup(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b4e38e-0926-4159-a83c-a1a85d30010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='4,5,6,7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac203d",
   "metadata": {},
   "source": [
    "## load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232e2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the seed for reproducibility\n",
    "seed = cfg.SEED + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53cd85a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DIST.DISTRIBUTED,\n",
    "cfg.DIST.DIST_BACKEND,\n",
    "cfg.DIST.GPU,\n",
    "cfg.DIST.WORLD_SIZE,\n",
    "cfg.TRAIN.BATCH_SIZE,\n",
    "cfg.DATASET.NUM_CLASSES\n",
    "cfg.DEBUG # check debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2225dd-8a11-4513-b328-d51a6cbac252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DIST.DISTRIBUTED, cfg.DIST.GPU, cfg.DIST.WORLD_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1502d5-5399-4eaf-acd2-fe1edb47b66f",
   "metadata": {},
   "source": [
    "## set device manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee2f500-9bd7-40a3-a1a2-9210a3f39fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f2471a5-612b-4ba7-9988-7e57217d9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(7)\n",
    "# note that torch.cuda.device() is used as a context manager\n",
    "# device = torch.device('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa7656a-91c7-4e1f-899f-f59b1020fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current device will always be zero even though the devices to which cuda is exposed \n",
    "# has been specified\n",
    "# torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7792bd38-dd94-4855-b13b-ce30cbb60238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok\n",
    "device = torch.device('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a73949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.TRAIN.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95373cc",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c821f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/users/cku/anaconda3/envs/deformable_detr/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch2/users/cku/anaconda3/envs/deformable_detr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46836820"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model and send it to cuda device\n",
    "model, criterion, postprocessors, postprocessors_target = build_model(cfg)\n",
    "\n",
    "# cannot set multiple devices at once, can only specify multiple devices when calling data parallel or distributed data parallel\n",
    "\n",
    "# with torch.cuda.device(0):\n",
    "model.to(device) \n",
    "    \n",
    "model_without_ddp = model\n",
    "# model=torch.nn.parallel.DistributedDataParallel(model, device_ids=[0])\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8f4f7",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f7f9e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# takes a long time, write in a separate cell\n",
    "# dataset_train = build_dataset(image_set='train', cfg=cfg)\n",
    "dataset_val = build_dataset(image_set='val', cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1123e",
   "metadata": {},
   "source": [
    "## build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3de3fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampler for training data\n",
    "# sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "# # for uda only: if not //2 we only get half of the dataset\n",
    "# batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "#             sampler_train, cfg.TRAIN.BATCH_SIZE//2, drop_last=True)\n",
    "\n",
    "# random sampler for training\n",
    "# data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "#                                collate_fn=DAOD.collate_fn, num_workers=cfg.NUM_WORKERS,\n",
    "#                              pin_memory=True)\n",
    "\n",
    "# build batch sampler\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "\n",
    "# the valid dataloder uses a sequential sampler\n",
    "data_loader_val = DataLoader(dataset_val, cfg.TRAIN.BATCH_SIZE, sampler=sampler_val,\n",
    "                             drop_last=False, collate_fn=utils.collate_fn, num_workers=cfg.NUM_WORKERS,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2588793",
   "metadata": {},
   "source": [
    "## build data loader subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075f434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset\n",
    "indices = torch.arange(10)\n",
    "dataset_train_subset = torch.utils.data.Subset(dataset_train,indices)\n",
    "\n",
    "# remember to divide batch size by 2\n",
    "data_loader_train_subset = DataLoader(dataset_train_subset, cfg.TRAIN.BATCH_SIZE//2,\n",
    "                            collate_fn=DAOD.collate_fn, num_workers=cfg.NUM_WORKERS,\n",
    "                             pin_memory=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c41d7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader_train_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check number of batches\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_loader_train_subset\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader_train_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# check number of batches\n",
    "len(data_loader_train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f99548",
   "metadata": {},
   "source": [
    "## load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efd0f384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'exps_bs4_retrain/AQT_pretrain_multi_scale/checkpoint0095.pth'\n",
    "# load model weights\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "model_without_ddp.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf466ed9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeformableDETR(\n",
       "  (memory): Memory()\n",
       "  (transformer): DeformableTransformer(\n",
       "    (encoder): DeformableTransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DeformableTransformerEncoderLayer(\n",
       "          (space_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (channel_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (self_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DeformableTransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DeformableTransformerDecoderLayer(\n",
       "          (instance_attn): DomainAttention(\n",
       "            (grl): GradientReversal()\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (cross_attn): MSDeformAttn(\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout4): Dropout(p=0.1, inplace=False)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (bbox_embed): ModuleList(\n",
       "        (0): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (reference_points): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (channel_query): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (grl): GradientReversal()\n",
       "  )\n",
       "  (class_embed): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (3): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (4): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (5): Linear(in_features=256, out_features=9, bias=True)\n",
       "  )\n",
       "  (bbox_embed): ModuleList(\n",
       "    (0): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(300, 512)\n",
       "  (input_proj): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (backbone): ResNet(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "        (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "      )\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       "  (space_D): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (channel_D): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (instance_D): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose to capture outputs at train or test mode\n",
    "model.eval()\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b48405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "597e5ba9",
   "metadata": {},
   "source": [
    "## collect backbone features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4be54ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import box_ops\n",
    "import torchvision\n",
    "from datasets.data_prefetcher import data_prefetcher\n",
    "from models.utils import weighted_aggregate_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8931c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = iter(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43c323a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = cfg.TRAIN.BATCH_SIZE\n",
    "scale = 1/32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1f28d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
    "samples, targets = prefetcher.next() # samples have been transformed at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7490e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the input projection layers \n",
    "# different layer features\n",
    "for l, feat in enumerate(features):\n",
    "    src, mask = feat.decompose()\n",
    "    srcs.append(self.input_proj[l](src))\n",
    "    masks.append(mask)\n",
    "    assert mask is not None\n",
    "\n",
    "if self.num_feature_levels > len(srcs):\n",
    "    _len_srcs = len(srcs)\n",
    "    # num_feature_levels = 4 by defualt\n",
    "    for l in range(_len_srcs, self.num_feature_levels):\n",
    "\n",
    "        # one feature level\n",
    "        if l == _len_srcs:\n",
    "            src = self.input_proj[l](features[-1].tensors)\n",
    "        else:\n",
    "            src = self.input_proj[l](srcs[-1])\n",
    "        m = samples.mask\n",
    "        mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]\n",
    "        pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)\n",
    "        srcs.append(src)\n",
    "        masks.append(mask)\n",
    "        pos.append(pos_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1835aab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# rois_all = []\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# src_labels = []\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# src_scores = []\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdata_loader_train\u001b[49m, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocess src rois\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     samples = samples.to(device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         rescaled_boxes_enc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m         list_of_labels_enc \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader_train' is not defined"
     ]
    }
   ],
   "source": [
    "# rois_all = []\n",
    "# src_labels = []\n",
    "# src_scores = []\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(data_loader_train, desc = 'preprocess src rois'):\n",
    "\n",
    "    #     samples = samples.to(device)\n",
    "    #     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        rescaled_boxes_enc = []\n",
    "        list_of_labels_enc = []\n",
    "        list_of_scores_enc = []\n",
    "\n",
    "        # collect boxes\n",
    "        for batch_idx in range(0, B//2, 1):\n",
    "            source_boxes = targets[batch_idx]['boxes']\n",
    "            source_labels = targets[batch_idx]['labels'].tolist()\n",
    "            source_scores = torch.ones(source_boxes.shape[0]).cuda()\n",
    "            # source_scores, _ = torch.max(outputs_class_conf[batch_idx][keep_tmp], dim=1)\n",
    "\n",
    "            boxes_rescaled = box_ops.box_cxcywh_to_xyxy(source_boxes) # src only, batch size = 1\n",
    "            # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "            # img_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
    "            img_sizes = targets[batch_idx][\"size\"]\n",
    "            img_h, img_w = img_sizes.unbind(0)\n",
    "            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=0) # different scale factor for different images\n",
    "\n",
    "            # scale all boxes with the corresponding image sizes\n",
    "            for b in range(boxes_rescaled.shape[0]):\n",
    "                boxes_rescaled[b] *= scale_fct\n",
    "\n",
    "            rescaled_boxes_enc.append(boxes_rescaled) # delist\n",
    "            list_of_labels_enc.append(source_labels)\n",
    "            list_of_scores_enc.append(source_scores)\n",
    "\n",
    "        # (B, feat_dim, h, w)\n",
    "        backbone_feat = model.backbone(samples)[0]\n",
    "\n",
    "        # single scale\n",
    "        backbone_feat_single = backbone_feat[0].tensors\n",
    "\n",
    "        # get src boxes\n",
    "        src_boxes = rescaled_boxes_enc[:B//2]\n",
    "        src_labels = list_of_labels_enc[:B//2]\n",
    "        src_scores = list_of_scores_enc[:B//2]\n",
    "\n",
    "        rois_list_per_sample = []\n",
    "        for batch_idx in range(0, B//2, 1):\n",
    "            # input dim: (N, C, H, W)\n",
    "            rois = torchvision.ops.roi_align(backbone_feat_single[batch_idx].unsqueeze(0), [src_boxes[batch_idx]], output_size=(7, 7), spatial_scale=scale, aligned=True).mean(3).mean(2)\n",
    "            rois_list_per_sample.append(rois)\n",
    "\n",
    "            \n",
    "#         rois_all.append(rois_list_per_sample)\n",
    "\n",
    "        list_of_src_prototype = [] # [scale], (num_classes, feat_dim)\n",
    "    \n",
    "        # along scale\n",
    "        for roi_group in rois_all:\n",
    "            # batch dim reduced after aggregation\n",
    "            # (e.g scale 1, bs 2 --> all reduced)\n",
    "            # (e.g scale 4, bs 2 --> 4, bs reduced)\n",
    "            \n",
    "            # (1,8,256)\n",
    "            src_prototypes_enc, _ = weighted_aggregate_tmp(B, src_labels, roi_group, src_scores, None, 8, 2048)\n",
    "            list_of_src_prototype.append(src_prototypes_enc)\n",
    "\n",
    "\n",
    "        samples, targets = prefetcher.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d63860dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1487"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois_all.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98335047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois_all[2].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21fa3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rois_all, 'preprocessed_src_rois.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_all = torch.load('preprocessed_src_rois.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ed7b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m list_of_src_prototype \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# [scale], (num_classes, feat_dim)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m roi_group \u001b[38;5;129;01min\u001b[39;00m rois_all:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# batch dim reduced after aggregation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# (e.g scale 1, bs 2 --> all reduced)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# (e.g scale 4, bs 2 --> 4, bs reduced)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     src_prototypes_enc, _ \u001b[38;5;241m=\u001b[39m weighted_aggregate_tmp(B, \u001b[43msrc_labels\u001b[49m, roi_group, src_scores, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[1;32m      8\u001b[0m     list_of_src_prototype\u001b[38;5;241m.\u001b[39mappend(src_prototypes_enc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_labels' is not defined"
     ]
    }
   ],
   "source": [
    "B = cfg.TRAIN.BATCH_SIZE\n",
    "list_of_src_prototype = [] # [scale], (num_classes, feat_dim)\n",
    "for roi_group in rois_all:\n",
    "    # batch dim reduced after aggregation\n",
    "    # (e.g scale 1, bs 2 --> all reduced)\n",
    "    # (e.g scale 4, bs 2 --> 4, bs reduced)\n",
    "    src_prototypes_enc, _ = weighted_aggregate_tmp(B, src_labels, roi_group, src_scores, None, self.num_classes, self.hidden_dim)\n",
    "    list_of_src_prototype.append(src_prototypes_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fca92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1b3c4d5",
   "metadata": {},
   "source": [
    "## gradcam visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from util.plot_utils import inverse_transform\n",
    "from datasets.data_prefetcher import data_prefetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loader_train_subset)\n",
    "data = next(data_iter) # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples are always from the first index, then targets\n",
    "samples = data[0]\n",
    "targets = data[1]\n",
    "\n",
    "# samples = data[0].tensors.shape\n",
    "# targets.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for samples, targets in tqdm(data_loader_val, desc='running inference'):\n",
    "samples = samples.to(device)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "outputs = model(samples, targets, None, None, None)\n",
    "\n",
    "out, features, memory, hs, ema_prototypes = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_prototypes[1].backward()\n",
    "memory_gradients = model.get_activations_gradient() # in cam viz mode, this will compute the gradients of the desired feature activation\n",
    "memory_activations = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for samples, targets in tqdm(data_loader_val, desc='running inference'):\n",
    "# samples = samples.to(device)\n",
    "# targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "# outputs = model(samples, targets, None, None, None)\n",
    "# out, features, memory, hs, ema_prototypes = outputs\n",
    "\n",
    "ema_prototypes[1].backward()\n",
    "memory_gradients = model.get_activations_gradient() # in cam viz mode, this will compute the gradients of the desired feature activation\n",
    "memory_activations = memory\n",
    "\n",
    "# pool gradients\n",
    "pooled_memory_gradients = torch.mean(memory_gradients, dim=[0,1])\n",
    "# weight activations with mean gradients\n",
    "memory_activations= memory_activations*pooled_memory_gradients.unsqueeze(-1)\n",
    "# mean across channel dim for visualization\n",
    "memory_activations = torch.mean(memory_activations, dim=-1)\n",
    "# last layer features shape\n",
    "B, c, h, w = features[-1].tensors.shape\n",
    "memory_activations = memory_activations.reshape(B,h,w)\n",
    "\n",
    "# relu on heatmap\n",
    "heatmap = np.maximum(memory_activations, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4907d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize heatmap on original image\n",
    "import cv2\n",
    "img = cv2.imread('./data/Elephant/data/05fig34.jpg')\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "cv2.imwrite('./map.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b96410",
   "metadata": {},
   "source": [
    "## simple attention map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from util.plot_utils import inverse_transform\n",
    "from datasets.data_prefetcher import data_prefetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_attention_ratio(img_path, attention_mask, ratio=0.5, cmap=\"jet\"):\n",
    "    \"\"\"\n",
    "    img_path: 读取图片的位置\n",
    "    attention_mask: 2-D 的numpy矩阵\n",
    "    ratio:  放大或缩小图片的比例，可选\n",
    "    cmap:   attention map的style，可选\n",
    "    \"\"\"\n",
    "    print(\"load image from: \", img_path)\n",
    "    # load the image\n",
    "    img = Image.open(img_path, mode='r')\n",
    "    img_h, img_w = img.size[0], img.size[1]\n",
    "    plt.subplots(nrows=1, ncols=1, figsize=(0.02 * img_h, 0.02 * img_w))\n",
    "\n",
    "    # scale the image\n",
    "    img_h, img_w = int(img.size[0] * ratio), int(img.size[1] * ratio)\n",
    "    img = img.resize((img_h, img_w))\n",
    "    plt.imshow(img, alpha=1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # normalize the attention mask\n",
    "    mask = cv2.resize(attention_mask, (img_h, img_w))\n",
    "    normed_mask = mask / mask.max()\n",
    "    normed_mask = (normed_mask * 255).astype('uint8')\n",
    "    plt.imshow(normed_mask, alpha=0.5, interpolation='nearest', cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(feature_map, boxes, labels):\n",
    "    '''\n",
    "    feature_map: encoder feature map (feat_dim, h, w)\n",
    "    \n",
    "    boxes: gt bounding boxes [bs] (num_proposals, 4)\n",
    "    \n",
    "    labels: gt labels [bs][num_proposals]\n",
    "    \n",
    "    '''\n",
    "    CLASSES = ['person','car','train','rider','truck','motorcycle','bicycle', 'bus'] # whole set\n",
    "\n",
    "    plt.figure(figsize=(feature_map.shape[-2],feature_map.shape[-1]))\n",
    "    plt.imshow(feature_map[0].max()[0])\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        class_idx = label\n",
    "        \n",
    "        for (xmin, ymin, xmax, ymax) in box:\n",
    "\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                    fill=False, color='g', linewidth=1))\n",
    "\n",
    "            text = f'{CLASSES[class_idx-1]}: {class_idx:0.2f}'\n",
    "\n",
    "            ax.text(xmin, ymin, text, fontsize=15,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = cfg.TRAIN.BATCH_SIZE\n",
    "\n",
    "# get prototypes\n",
    "# file_name = 'exps_bs4_retrain/test1_bg_and_fixed_thresh_calibrated_balanced_weight/memory_prototypes/ema_prototypes_epoch_0090.pt'\n",
    "file_name = 'exps_bs4_retrain/test1_bg_and_fixed_thresh_calibrated_balanced_weight/memory_prototypes/ema_prototypes_epoch_0118.pt'\n",
    "\n",
    "\n",
    "prototypes = torch.load(file_name) # (num_class, feat_dim)\n",
    "prototypes = prototypes.cuda()\n",
    "src_prototype = prototypes[0] # (8, 256)\n",
    "tgt_prototype = prototypes[1] # (8, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUG: DAOD dataset makes batch index a list type when applying iter() on the dataloader\n",
    "# prefetch data\n",
    "# prefetcher = data_prefetcher(data_loader_train, device, prefetch=True)\n",
    "# samples, targets = prefetcher.next() # samples have been transformed at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "data_loader_train = iter(data_loader_train_subset)\n",
    "samples_and_targets = next(data_loader_train)\n",
    "samples = samples_and_targets[0]\n",
    "targets = samples_and_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baf198",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.tensors.shape, targets.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5be4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### send data to cuda devices\n",
    "sample = samples.to(device) # tensor, need to be a nested tensor before foward pass\n",
    "target = [{k: v.to(device) for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get target gt boxes for visualization\n",
    "from util import box_ops\n",
    "\n",
    "target_boxes_list = []\n",
    "target_labels_list = []\n",
    "### get proposal boxes\n",
    "for batch_idx in range(B//2, B, 1):\n",
    "    target_gt_boxes = target[batch_idx]['boxes']\n",
    "    target_gt_labels = target[batch_idx]['labels'].tolist()\n",
    "    target_gt_scores = torch.ones(target_gt_boxes.shape[0]).cuda()\n",
    "    \n",
    "    boxes_rescaled = box_ops.box_cxcywh_to_xyxy(target_gt_boxes)\n",
    "    # and from relative [0, 1] to absolute [0, height] coordinates\n",
    "    img_sizes = target[batch_idx][\"size\"]\n",
    "    img_h, img_w = img_sizes.unbind(0)\n",
    "\n",
    "    # since box tensor is (x,y,x,y)\n",
    "    scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=0)\n",
    "    \n",
    "    # scale boxes back to original sizes\n",
    "    for b in range(boxes_rescaled.shape[0]):\n",
    "        boxes_rescaled[b] *= scale_fct[0] # batch_size = 1, one image\n",
    "\n",
    "    target_boxes_list.append(boxes_rescaled)\n",
    "    target_labels_list.append(target_gt_labels)\n",
    "#     target_scores, _ = torch.max(outputs_class_conf[batch_idx][keep_tmp], dim=1)\n",
    "#     target_scores_list.append(target_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "### forward\n",
    "outputs = model(sample, target, 1, 1, 1)\n",
    "out, features, memory, hs, _ = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_feat = memory[-1] # use last sample\n",
    "memory_feat = memory_feat.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_prototype_src = src_prototype[1] # (,256) car prototype\n",
    "# class_prototype_tgt = tgt_prototype[1]\n",
    "\n",
    "class_prototype_src = src_prototype[0] # (,256) person prototype\n",
    "class_prototype_tgt = tgt_prototype[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute similarity\n",
    "# class_prototype_src_reshaped = class_prototype_src.expand(2,1,class_prototype_src.shape[-1]) # (bs, feat_dim, token_num)\n",
    "# memory_reshaped = memory.flatten(2,3).permute(0,2,1)\n",
    "# attn_mask = torch.matmul(memory_reshaped, class_prototype_src_reshaped.transpose(2,1)) # memory: (bs, feat_dim, token_num)\n",
    "# attn_mask = attn_mask.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use conv2d to compute similarity\n",
    "\n",
    "class_filter_src = class_prototype_src.view(1, class_prototype_src.shape[-1], 1, 1) # (num_class, feat, h, w)\n",
    "class_filter_tgt = class_prototype_tgt.view(1, class_prototype_tgt.shape[-1], 1, 1) # (num_class, feat, h, w)\n",
    "\n",
    "# memory = memory.squeeze(-1) #  (bs, token_num, feat_dim, 1)\n",
    "\n",
    "attn_mask_src = F.conv2d(memory_feat, class_filter_src) # (1, h, w)\n",
    "attn_mask_tgt = F.conv2d(memory_feat, class_filter_tgt) # (1, h, w)\n",
    "\n",
    "attn_mask_src = attn_mask_src.squeeze(0).squeeze(0) # (h,w)\n",
    "attn_mask_tgt = attn_mask_tgt.squeeze(0).squeeze(0) # (h,w)\n",
    "\n",
    "# # w/ src prototype\n",
    "# # attn_mask_source_image = attn_mask_src[0].squeeze()\n",
    "# attn_mask_target_image = attn_mask_src[1].squeeze()\n",
    "\n",
    "# # w/ target prototype\n",
    "# # attn_mask_source_image_tgt = attn_mask_tgt[0].squeeze()\n",
    "# attn_mask_target_image_tgt = attn_mask_tgt[1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6749f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask_src.max(), attn_mask_src.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2aae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask_tgt.shape, attn_mask_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe59265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# plot preprocessed train images\n",
    "unmasked_samples = sample.tensors[-1][:, :target[1]['size'][0], :target[1]['size'][1]]\n",
    "inverted_image_tensors = inverse_transform(unmasked_samples)\n",
    "inverted_image_tensors = inverted_image_tensors.permute(1,2,0)\n",
    "inverted_image_tensors = inverted_image_tensors.cpu().detach().numpy() # ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4529ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "attn_mask_src_numpy = attn_mask_src.detach().cpu().numpy()\n",
    "attn_mask_tgt_numpy = attn_mask_tgt.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try uin8\n",
    "inverted_image_tensors_uin8 = np.uint8(inverted_image_tensors*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize heat map\n",
    "heatmap_src = cv2.resize(attn_mask_src_numpy, (inverted_image_tensors.shape[1], inverted_image_tensors.shape[0]))\n",
    "heatmap_src = np.uint8(255 * heatmap_src)\n",
    "heatmap_src = cv2.applyColorMap(heatmap_src, cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize heat map\n",
    "heatmap_tgt = cv2.resize(attn_mask_tgt_numpy, (inverted_image_tensors.shape[1], inverted_image_tensors.shape[0]))\n",
    "heatmap_tgt = np.uint8(255 * heatmap_tgt)\n",
    "heatmap_tgt = cv2.applyColorMap(heatmap_tgt, cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert both type into numpy array, otherwise there will be a concat error\n",
    "superimposed_img_src = heatmap_src * 0.4 + inverted_image_tensors_uin8\n",
    "cv2.imwrite('./target_img_w_src_proto.jpg', superimposed_img_src)\n",
    "\n",
    "# plt.figure(figsize=(30, 50))\n",
    "# plt.imshow(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de03ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert both type into numpy array, otherwise there will be a concat error\n",
    "superimposed_img_tgt = heatmap_tgt * 0.4 + inverted_image_tensors_uin8\n",
    "cv2.imwrite('./target_img_w_tgt_proto.jpg', superimposed_img_tgt)\n",
    "\n",
    "# plt.figure(figsize=(30, 50))\n",
    "# plt.imshow(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize heatmap on original image\n",
    "# import cv2\n",
    "# img = cv2.imread('./data/Elephant/data/05fig34.jpg')\n",
    "# heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "# heatmap = np.uint8(255 * heatmap)\n",
    "# heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "# superimposed_img = heatmap * 0.4 + img\n",
    "# cv2.imwrite('./map.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attn(attn_mask):\n",
    "    # normalize attention mask\n",
    "    attn_mask_normalized = attn_mask/attn_mask.max()\n",
    "    attn_mask_visible = attn_mask_normalized.cpu().detach().numpy()\n",
    "    # attn_mask = (attn_mask * 255).cpu().detach().numpy().astype('uint8')\n",
    "\n",
    "#     cmap = 'jet'\n",
    "    plt.figure(figsize=(10,20))\n",
    "#     plt.imshow(attn_mask_visible, alpha=0.9, interpolation='nearest', cmap=cmap)\n",
    "    plt.imshow(attn_mask_visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attn(attn_mask[1].reshape(memory.shape[-2], memory.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original feature map from pretrained AQT\n",
    "visualize_attn(memory[1].max(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2298290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(feature_map, boxes, labels):\n",
    "    '''\n",
    "    feature_map: encoder feature map (feat_dim, h, w)\n",
    "    \n",
    "    boxes: gt bounding boxes [bs] (num_proposals, 4)\n",
    "    \n",
    "    labels: gt labels [bs][num_proposals]\n",
    "    \n",
    "    '''\n",
    "    CLASSES = ['person','car','train','rider','truck','motorcycle','bicycle', 'bus'] # whole set\n",
    "    \n",
    "    # feature_map[0]: source feature map\n",
    "    # feature_map[1]: target feature map\n",
    "    \n",
    "    feature_map_np = feature_map[0].max(0)[0].cpu().detach().numpy() # convert to numpy\n",
    "    plt.figure(figsize=(feature_map.shape[-2],feature_map.shape[-1]))\n",
    "    plt.imshow(feature_map_np)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        class_idx = label\n",
    "        import pdb; pdb.set_trace()\n",
    "        for box_i in range(box.shape[0]):\n",
    "            \n",
    "            xmin = box[box_i][0].cpu().detach().numpy()\n",
    "            ymin = box[box_i][1].cpu().detach().numpy()\n",
    "            xmax = box[box_i][2].cpu().detach().numpy()\n",
    "            ymax = box[box_i][3].cpu().detach().numpy()\n",
    "            \n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                    fill=False, color='g', linewidth=1))\n",
    "\n",
    "            text = f'{CLASSES[class_idx-1]}: {class_idx:0.2f}'\n",
    "\n",
    "            ax.text(xmin, ymin, text, fontsize=15,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4655319",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxes(memory, target_boxes_list, target_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a36c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original feature map from pretrained AQT\n",
    "visualize_attn(memory[0].max(0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a42046",
   "metadata": {},
   "source": [
    "## visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot preprocessed train images\n",
    "unmasked_samples = sample.tensors[1][:, :target[1]['size'][0], :target[1]['size'][1]]\n",
    "inverted_image_tensors = inverse_transform(unmasked_samples)\n",
    "inverted_image_tensors = inverted_image_tensors.permute(1,2,0)\n",
    "inverted_image_tensors = inverted_image_tensors.cpu().detach()\n",
    "plt.figure(figsize=(30, 50))\n",
    "plt.imshow(inverted_image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_samples = sample.tensors[0][:, :target[1]['size'][0], :target[1]['size'][1]]\n",
    "inverted_image_tensors = inverse_transform(unmasked_samples)\n",
    "inverted_image_tensors = inverted_image_tensors.permute(1,2,0)\n",
    "inverted_image_tensors = inverted_image_tensors.cpu().detach()\n",
    "plt.figure(figsize=(30, 50))\n",
    "plt.imshow(inverted_image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source prototype on target image\n",
    "visualize_attn(attn_mask_target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ad7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target prototype on target image\n",
    "visualize_attn(attn_mask_target_image_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_samples = sample[1][:, :target[1]['size'][0], :target[1]['size'][1]]\n",
    "inverted_image_tensors = inverse_transform(unmasked_samples)\n",
    "inverted_image_tensors = inverted_image_tensors.permute(1,2,0)\n",
    "inverted_image_tensors = inverted_image_tensors.cpu().detach()\n",
    "plt.figure(figsize=(30, 50))\n",
    "plt.imshow(inverted_image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a92448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view corresponding sample image\n",
    "\n",
    "unpadded_samples = samples.tensors[0][:, :targets[0]['size'][0], :targets[0]['size'][1]]\n",
    "average_image = unpadded_samples.mean(0)\n",
    "inverted_image_tensors = inverse_transform(unpadded_samples)\n",
    "inverted_image_tensors = inverted_image_tensors.permute(1,2,0)\n",
    "plt.figure(figsize=(30, 50))\n",
    "plt.imshow(inverted_image_tensors.detach())\n",
    "plt.axis('off')\n",
    "plt.savefig(str('./visualization/image/image_{}.png').format(targets[0]['image_id'].cpu().item()), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5cb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22646f52",
   "metadata": {},
   "source": [
    "## train TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b76662d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch index and query index\n",
    "def _get_src_permutation_idx(indices):\n",
    "    # permute predictions following indices\n",
    "    batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "    src_idx = torch.cat([src for (src, _) in indices])\n",
    "    return batch_idx, src_idx\n",
    "\n",
    "def _get_tgt_permutation_idx(indices):\n",
    "    # permute targets following indices\n",
    "    batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "    tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "    return batch_idx, tgt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce67a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running inference:   0%|                                                   | 0/125 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# data and labels for inference\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m samples, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader_val, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning inference\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m     20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(samples, targets, cur_iter_num, total_iter_num, total_epoch) \u001b[38;5;66;03m# in debug mode, outputs would be a tuple\u001b[39;00m\n",
      "File \u001b[0;32m/scratch2/users/cku/adaptation/AQT_subset/util/misc.py:360\u001b[0m, in \u001b[0;36mNestedTensor.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# type: (Device) -> NestedTensor # noqa\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     cast_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# model.eval() # set to eval mode to get both source and target labels\n",
    "\n",
    "total_epoch = 0\n",
    "cur_iter_num = 0\n",
    "total_iter_num = 0 \n",
    "# length of dataloader is 250\n",
    "target_features_1 = []\n",
    "target_features_2 = []\n",
    "target_features_new = []\n",
    "tgt_label_1 = []\n",
    "tgt_label_2 = []\n",
    "tgt_label_new = []\n",
    "\n",
    "boxes = []\n",
    "\n",
    "# data and labels for inference\n",
    "for samples, targets in tqdm(data_loader_val, desc='running inference'):\n",
    "    samples = samples.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    outputs = model(samples, targets, cur_iter_num, total_iter_num, total_epoch) # in debug mode, outputs would be a tuple\n",
    "    \n",
    "    # out, features, memory, hs = outputs\n",
    "    out = outputs\n",
    "\n",
    "    loss_dict, indices = criterion(outputs, targets, mode='test', scale='single')\n",
    "    \n",
    "    # get matched query gt indices\n",
    "    # two sets of indices for train mode\n",
    "    idx_1 = _get_src_permutation_idx(indices) # get batch and permuted query position\n",
    "    \n",
    "    # target_classes: (2,300,9)\n",
    "    target_classes = torch.full((2,300), 9,\n",
    "                                dtype=torch.int64, device=out['pred_logits'].device)\n",
    "\n",
    "    \n",
    "    # all class labels across all samples within a batch\n",
    "    target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "    \n",
    "    target_classes[idx_1] = target_classes_o\n",
    "\n",
    "    # torch.Size([300])\n",
    "    # for eval mode, only target data is loaded\n",
    "    target_label_1 = target_classes[0]\n",
    "    \n",
    "    # exclude background\n",
    "    tgt_query_pos_1 = torch.where(target_label_1!=9)\n",
    "    \n",
    "    # get correponding labels for each non-background query\n",
    "    target_gt_1 = target_label_1[tgt_query_pos_1[0]]\n",
    "    \n",
    "    # you may visualise subset of classes here by excluding the classes\n",
    "#     target_gt_1_idx = torch.where(target_gt_1!=1)\n",
    "#     target_gt_1_new = target_gt_1[target_gt_1_idx]\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    # get src query embeddings\n",
    "    target_out_1 = torch.index_select(hs[-1][0], 0, tgt_query_pos_1[0]) #tgt_query_pos_1 is a tuple, thus need to index element\n",
    "#     target_gt_1 = torch.index_select(hs[-1][0], 0, target_gt_1_idx[0])\n",
    "#     target_gt_1 = torch.index_select(, 0, target_gt_1[0])\n",
    "#     import pdb; pdb.set_trace()\n",
    "\n",
    "    # store \n",
    "    target_features_1.extend(target_out_1.cpu().detach().numpy())\n",
    "    target_features_new.extend(target_gt_1.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    # accumulate for all samples\n",
    "    tgt_label_1.extend(target_gt_1.cpu().detach().numpy())\n",
    "#     tgt_label_new.extend(target_gt_1_new.cpu().detach().numpy())\n",
    "    \n",
    "    # target boxes\n",
    "#     boxes.append(rescaled_boxes[1].cpu().detach().numpy())\n",
    "\n",
    "target_features_1 = np.stack(target_features_1)\n",
    "# target_features_new = np.stack(target_features_new)\n",
    "tgt_label_1 = np.stack(tgt_label_1)\n",
    "# tgt_label_new = np.stack(tgt_label_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "# check lengths\n",
    "sum([len(list(group)) for key, group in groupby(sorted(tgt_label_1))])\n",
    "len([len(list(group)) for key, group in groupby(sorted(tgt_label_1))])\n",
    "len(target_features_1), len(tgt_label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tsne\n",
    "# tsne_tgt_new = TSNE(n_components=2).fit_transform(new_target_features_1)\n",
    "tsne_tgt_new = TSNE(n_components=2).fit_transform(target_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and move the coordinates so they fit [0; 1] range\n",
    "\n",
    "def scale_to_01_range(x):\n",
    "\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    "\n",
    "   # move the distribution so that it starts from zero\n",
    "   # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    "    # make the distribution fit [0; 1] by dividing by its range\n",
    "\n",
    "    return starts_from_zero / value_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9acb28",
   "metadata": {},
   "source": [
    "## tsne on tgt query embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e89fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tgt_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8db3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values for x and y\n",
    "tx_tgt = tsne_tgt_new[:, 0]\n",
    "ty_tgt = tsne_tgt_new[:, 1]\n",
    "\n",
    "# scale to range between 0 and 1\n",
    "tx_tgt = scale_to_01_range(tx_tgt)\n",
    "ty_tgt = scale_to_01_range(ty_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b102924",
   "metadata": {},
   "source": [
    "## restrict number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import groupby\n",
    "\n",
    "# number_samples= np.sort([len(list(group)) for key, group in groupby(sorted(tgt_label_1))])[1]\n",
    "\n",
    "# # mutable, so write in different loops\n",
    "# new_tgt_label_1 = []\n",
    "# for key, group in groupby(sorted(tgt_label_1)):\n",
    "# #     print(list(group)[:number_samples])\n",
    "# #     length = len(list(group))\n",
    "# #     new_target_features_1.extend(target_features_1[:length][:number_samples])\n",
    "#     new_tgt_label_1.extend(list(group)[:number_samples])\n",
    "\n",
    "# # new_tgt_label_1 = []\n",
    "# new_target_features_1 = []\n",
    "# for key, group in groupby(sorted(tgt_label_1)):\n",
    "# #     print(list(group)[:number_samples])\n",
    "#     length = len(list(group))\n",
    "#     new_target_features_1.extend(target_features_1[:length][:number_samples])\n",
    "# #     new_tgt_label_1.extend(list(group)[:number_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8e3fb",
   "metadata": {},
   "source": [
    "## contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16809e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an extra label for prototype tokens\n",
    "colors_per_class = {'0': 'r', '1': 'g', '2': 'b',\n",
    "          '3':'c', '4': 'm', '5': 'y',\n",
    "                    '6':'k', '7':'pink', '8':'w'}\n",
    "\n",
    "\n",
    "colors_per_class = {'1': 'g', '2': 'b', '4': 'm', '7':'pink'}\n",
    "\n",
    "# fir distinuishing decoder embeddings and prototypes \n",
    "marker = ['*']\n",
    "\n",
    "# need to use a different marker for prototype tokens\n",
    "\n",
    "# labels = []\n",
    "\n",
    "# need to know what labels are assigned to each feature in order to plot them\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot a cluster one by one\n",
    "for label in colors_per_class:\n",
    "\n",
    "    indices = [i for i, l in enumerate(tgt_label_1) if l == int(label)]\n",
    "    # use the indices of the current label to index the corresponding feature embeddings\n",
    "    current_tx = np.take(tx_tgt, indices)\n",
    "    current_ty = np.take(ty_tgt, indices)\n",
    "\n",
    "    # add a scatter plot with the corresponding color and label\n",
    "    ax.scatter(current_tx, current_ty, marker=marker[0], c=colors_per_class[label], label=label)\n",
    "        \n",
    "    \n",
    "# build a legend using the labels we set previously\n",
    "ax.legend(loc='best')\n",
    "# finally, show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dde8c",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f190f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an extra label for prototype tokens\n",
    "# colors_per_class = {'0': 'r', '1': 'g', '2': 'b',\n",
    "#           '3':'c', '4': 'm', '5': 'y',\n",
    "#                     '6':'k', '7':'pink', '8':'w'}\n",
    "\n",
    "\n",
    "colors_per_class = {'1': 'g', '2': 'b', '4': 'm', '7':'pink'}\n",
    "\n",
    "# fir distinuishing decoder embeddings and prototypes \n",
    "marker = ['*']\n",
    "\n",
    "# need to use a different marker for prototype tokens\n",
    "\n",
    "# labels = []\n",
    "\n",
    "# need to know what labels are assigned to each feature in order to plot them\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot a cluster one by one\n",
    "for label in colors_per_class:\n",
    "\n",
    "    indices = [i for i, l in enumerate(tgt_label_1) if l == int(label)]\n",
    "    # use the indices of the current label to index the corresponding feature embeddings\n",
    "    current_tx = np.take(tx_tgt, indices)\n",
    "    current_ty = np.take(ty_tgt, indices)\n",
    "\n",
    "    # add a scatter plot with the corresponding color and label\n",
    "    ax.scatter(current_tx, current_ty, marker=marker[0], c=colors_per_class[label], label=label)\n",
    "        \n",
    "    \n",
    "# build a legend using the labels we set previously\n",
    "ax.legend(loc='best')\n",
    "# finally, show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d881a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_label_1.shape, tgt_label_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192df49d",
   "metadata": {},
   "source": [
    "## plot memory embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef36e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and move the coordinates so they fit [0; 1] range\n",
    "\n",
    "def scale_to_01_range(x):\n",
    "\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    "\n",
    "   # move the distribution so that it starts from zero\n",
    "   # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    "    # make the distribution fit [0; 1] by dividing by its range\n",
    "\n",
    "    return starts_from_zero / value_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e26510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o positional update\n",
    "embeddings = torch.load('exps/200epochs/r50_uda_multi_scale_multi_layer_memory_size_40/keys0169.pt')\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "# perplexity: how to balance between local/global aspects of your data\n",
    "# it is also about the number of close neighbors each point has\n",
    "tsne = TSNE(n_components=2, perplexity=2).fit_transform(embeddings_np)\n",
    "\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "tx = scale_to_01_range(tx)\n",
    "ty = scale_to_01_range(ty)\n",
    "\n",
    "# need to know what labels are assigned to each feature in order to plot them\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add a scatter plot with the corresponding color and label\n",
    "ax.scatter(tx, ty)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aaa0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o positional update\n",
    "embeddings = torch.load('exps/200epochs/r50_uda_multi_scale_multi_layer_memory_partitioned/keys0169.pt')\n",
    "embeddings_np = embeddings.cpu().numpy()\n",
    "# perplexity: how to balance between local/global aspects of your data\n",
    "# it is also about the number of close neighbors each point has\n",
    "tsne = TSNE(n_components=2, perplexity=2).fit_transform(embeddings_np)\n",
    "\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    "tx = scale_to_01_range(tx)\n",
    "ty = scale_to_01_range(ty)\n",
    "\n",
    "# need to know what labels are assigned to each feature in order to plot them\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add a scatter plot with the corresponding color and label\n",
    "ax.scatter(tx, ty)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd222e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors_per_class = {'1': np.array([0.000, 0.447, 0.741]), '2': [0.850, 0.325, 0.098], '3': [0.929, 0.694, 0.125],\n",
    "#           '4': [0.494, 0.184, 0.556], '5': [0.466, 0.674, 0.188], '6': [0.301, 0.745, 0.933],\n",
    "#                     '7':[0.453, 0.233, 0.763], '8':[0.333, 0.674, 0.000]}\n",
    "\n",
    "\n",
    "colors_per_class = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'pink', 'orange']\n",
    "\n",
    "\n",
    "\n",
    "# supposedly this list should indicate the class id for each sample, but here we have a set\n",
    "labels = ['1','2','3','4','5','6','7','8']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deformable_detr",
   "language": "python",
   "name": "deformable_detr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
